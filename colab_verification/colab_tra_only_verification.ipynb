{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRA-Only Colab Verification\n",
        "## Complete Model Testing on Real Data\n",
        "\n",
        "**Purpose**: Verify ALL models work with real TRA turbulence data\n",
        "\n",
        "**Models Tested** (13 total):\n",
        "- **Neural Operators (4)**: FNO, TNO, UNet, DeepONet\n",
        "- **NO + Diffusion (4)**: FNO+DM, TNO+DM, UNet+DM, DeepONet+DM\n",
        "- **Legacy Deterministic (3)**: ResNet, Dil-ResNet, Latent-MGN\n",
        "- **Legacy Diffusion (2)**: ACDM, Refiner\n",
        "\n",
        "**Dataset**: TRA (128_small_tra) - 287 MB of real turbulence data from TUM server\n",
        "\n",
        "**Why TRA-only?**\n",
        "- Real data with proven format (no synthetic data issues)\n",
        "- Tests complete pipeline: download \u2192 train \u2192 checkpoints\n",
        "- Verifies all model architectures work\n",
        "\n",
        "**Expected Runtime**: ~4-6 hours on T4 GPU (13 models \u00d7 50 epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 0: Clean Slate (Optional)\n",
        "\n",
        "Run this if you've updated code or want to start fresh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 0: Clean Slate\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"\ud83d\uddd1\ufe0f  Clean Slate - Delete All Data\")\n",
        "confirm = input(\"Type 'DELETE' to confirm: \")\n",
        "\n",
        "if confirm == \"DELETE\":\n",
        "    for path in [Path('/content/Generatively-Stabilised-NOs'), Path('/content/colab_progress')]:\n",
        "        if path.exists():\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"\u2705 Deleted {path}\")\n",
        "    print(\"\\n\u2705 Ready to start fresh! Run Cell 1 next.\")\n",
        "else:\n",
        "    print(\"\u274c Cancelled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\ud83d\ude80 TRA-Only Verification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ensure we're in /content\n",
        "try:\n",
        "    os.chdir('/content')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clone repository\n",
        "repo_path = Path('/content/Generatively-Stabilised-NOs')\n",
        "if not repo_path.exists():\n",
        "    print(\"\ud83d\udce5 Cloning repository...\")\n",
        "    !git clone https://github.com/maximbeekenkamp/Generatively-Stabilised-NOs.git\n",
        "    print(\"\u2705 Repository cloned\")\n",
        "else:\n",
        "    print(\"\u2705 Repository exists\")\n",
        "\n",
        "%cd /content/Generatively-Stabilised-NOs\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
        "# Fix protobuf compatibility with Python 3.13+ and tensorboard\n",
        "!pip install -q neuraloperator matplotlib seaborn rich einops scipy pyyaml \"protobuf>=3.20.0,<4.0.0\" tensorboard\n",
        "print(\"\u2705 Dependencies installed\")\n",
        "\n",
        "# Setup paths\n",
        "sys.path.insert(0, str(repo_path))\n",
        "\n",
        "# Create progress tracking\n",
        "progress_dir = Path('/content/colab_progress')\n",
        "progress_dir.mkdir(exist_ok=True)\n",
        "(progress_dir / 'checkpoints').mkdir(exist_ok=True)\n",
        "(progress_dir / 'predictions').mkdir(exist_ok=True)\n",
        "\n",
        "progress_file = progress_dir / 'progress.json'\n",
        "if progress_file.exists():\n",
        "    with open(progress_file, 'r') as f:\n",
        "        progress = json.load(f)\n",
        "else:\n",
        "    progress = {'training': {}, 'predictions': {}}\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump(progress, f, indent=2)\n",
        "\n",
        "print(\"\\n\u2705 Setup complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: GPU Check\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"=\"*60)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\u2705 GPU: {gpu_name}\")\n",
        "    print(f\"   VRAM: {gpu_memory:.1f} GB\")\n",
        "    BATCH_SIZE = 4\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  No GPU - will be slow\")\n",
        "    BATCH_SIZE = 1\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Download TRA Data\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca DOWNLOADING TRA DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# FTP credentials (replace with actual)\n",
        "FTP_URL = 'ftp://USERNAME:PASSWORD@dataserv.ub.tum.de:21/128_tra_small.zip'\n",
        "\n",
        "zip_path = Path('data/128_tra_small.zip')\n",
        "zip_path.parent.mkdir(exist_ok=True)\n",
        "\n",
        "if not zip_path.exists():\n",
        "    print(\"\ud83d\udce5 Downloading 287 MB via FTP...\")\n",
        "    cmd = f\"curl -o {zip_path} '{FTP_URL}'\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\u274c Download failed: {result.stderr[:200]}\")\n",
        "        raise Exception(\"Download failed\")\n",
        "    print(f\"\u2705 Downloaded: {zip_path.stat().st_size / (1024**2):.0f} MB\")\n",
        "else:\n",
        "    print(f\"\u2705 Already downloaded: {zip_path.stat().st_size / (1024**2):.0f} MB\")\n",
        "\n",
        "# Extract\n",
        "if not Path('data/128_small_tra').exists():\n",
        "    print(\"\ud83d\udce6 Extracting...\")\n",
        "    cmd = f\"unzip -q -o {zip_path.absolute()} -d {Path('data').absolute()}\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\u274c Extraction failed: {result.stderr[:200]}\")\n",
        "        raise Exception(\"Extraction failed\")\n",
        "    print(\"\u2705 Extracted to data/128_small_tra/\")\n",
        "else:\n",
        "    print(\"\u2705 Already extracted\")\n",
        "\n",
        "# Verify\n",
        "tra_dir = Path('data/128_small_tra/sim_000000')\n",
        "if tra_dir.exists():\n",
        "    num_files = len(list(tra_dir.glob('*.npz')))\n",
        "    print(f\"\\n\u2705 TRA data ready: {num_files} files in sim_000000\")\n",
        "else:\n",
        "    print(\"\u274c TRA directory not found!\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Phase\n",
        "\n",
        "Training ALL 13 models on TRA dataset:\n",
        "- **Neural Operators (4)**: FNO, TNO, UNet, DeepONet\n",
        "- **NO + Diffusion (4)**: FNO+DM, TNO+DM, UNet+DM, DeepONet+DM\n",
        "- **Legacy Deterministic (3)**: ResNet, Dil-ResNet, Latent-MGN\n",
        "- **Legacy Diffusion (2)**: ACDM, Refiner\n",
        "\n",
        "Each model trains for 50 epochs on full dataset with batch size 4.\n",
        "\n",
        "**Total**: 13 models \u00d7 50 epochs \u2248 4-6 hours on T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Train Models (TRA Only)\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Re-add paths\n",
        "project_root = Path('/content/Generatively-Stabilised-NOs')\n",
        "sys.path.insert(0, str(project_root))\n",
        "os.chdir(project_root)\n",
        "\n",
        "from src.core.data_processing.turbulence_dataset import TurbulenceDataset\n",
        "from src.core.data_processing.data_transformations import Transforms\n",
        "from src.core.utils.params import DataParams, TrainingParams, LossParams, ModelParamsDecoder, ModelParamsEncoder, ModelParamsLatent\n",
        "from src.core.models.model import PredictionModel\n",
        "from src.core.training.loss import PredictionLoss\n",
        "from src.core.training.trainer import Trainer\n",
        "from src.core.training.loss_history import LossHistory\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca TRAINING MODELS (TRA ONLY)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# GPU-friendly training configuration\n",
        "EPOCHS = 50  # More epochs for GPU training (vs 3 for local CPU)\n",
        "BATCH_SIZE = 4  # Higher batch size for GPU (vs 1 for local CPU)\n",
        "\n",
        "# TRA configuration (matches local_tra_verification.py)\n",
        "TRA_CONFIG = {\n",
        "    'filter_top': ['128_small_tra'],\n",
        "    'filter_sim': [(0, 1)],\n",
        "    'filter_frame': [(0, 1000)],  # Full frame range\n",
        "    'sim_fields': ['dens', 'pres'],\n",
        "    'sim_params': ['mach'],  # CRITICAL: Only 'mach', not 'rey'! Affects channel count.\n",
        "    'normalize_mode': 'machMixed'  # Autoreg uses machMixed, not traMixed\n",
        "}\n",
        "\n",
        "# Models to test (13 models matching local_tra_verification.py)\n",
        "MODELS = {\n",
        "    # Neural Operators (Standalone)\n",
        "    'fno': {'arch': 'fno', 'dec_width': 56, 'fno_modes': (16, 8)},\n",
        "    'tno': {'arch': 'tno+Prev', 'dec_width': 256, 'tno_teacher_forcing_ratio': 0.6},  # L=2, K=4, 60% teacher forcing\n",
        "    'unet': {'arch': 'unet', 'dec_width': 96},\n",
        "    'deeponet': {'arch': 'deeponet', 'dec_width': 96, 'n_sensors': 392,\n",
        "                 'branch_batch_norm': True, 'trunk_batch_norm': True},\n",
        "\n",
        "    # Neural Operators + Diffusion Models (Generative Operators) - Stage 1: prior-only training\n",
        "    'fno_dm': {'arch': 'genop-fno-diffusion', 'dec_width': 56, 'fno_modes': (16, 8), 'diff_steps': 20, 'training_stage': 1,\n",
        "               'load_pretrained_prior': True, 'prior_checkpoint_key': 'fno_tra'},\n",
        "    'tno_dm': {'arch': 'genop-tno-diffusion+Prev', 'dec_width': 256, 'diff_steps': 20, 'training_stage': 1,\n",
        "               'load_pretrained_prior': True, 'prior_checkpoint_key': 'tno_tra', 'tno_teacher_forcing_ratio': 0.6},\n",
        "    'unet_dm': {'arch': 'genop-unet-diffusion', 'dec_width': 96, 'diff_steps': 20, 'training_stage': 1,\n",
        "                'load_pretrained_prior': True, 'prior_checkpoint_key': 'unet_tra'},\n",
        "    'deeponet_dm': {'arch': 'genop-deeponet-diffusion', 'dec_width': 96, 'diff_steps': 20, 'training_stage': 1, 'n_sensors': 392,\n",
        "                    'load_pretrained_prior': True, 'prior_checkpoint_key': 'deeponet_tra',\n",
        "                    'branch_batch_norm': True, 'trunk_batch_norm': True},\n",
        "\n",
        "    # Legacy Deterministic\n",
        "    'resnet': {'arch': 'resnet', 'dec_width': 144},\n",
        "    'dil_resnet': {'arch': 'dil_resnet', 'dec_width': 144},\n",
        "    'latent_mgn': {'arch': 'skip', 'dec_width': 96, 'enc_width': 32, 'latent_size': 32,\n",
        "                   'requires_encoder': True, 'requires_latent': True, 'vae': False},\n",
        "\n",
        "    # Legacy Diffusion (Standalone)\n",
        "    'acdm': {'arch': 'direct-ddpm+Prev', 'diff_steps': 20, 'sequence_length': [3, 2],\n",
        "             'is_diffusion_model': True, 'diff_cond_integration': 'noisy'},\n",
        "    'refiner': {'arch': 'refiner', 'diff_steps': 4, 'refiner_std': 0.000001,\n",
        "                'is_diffusion_model': True, 'dec_width': 96},\n",
        "}\n",
        "\n",
        "def create_model_params(config, checkpoint_config=None):\n",
        "    '''Create model parameters matching local_tra_verification.py\n",
        "\n",
        "    Args:\n",
        "        config: Model configuration dictionary\n",
        "        checkpoint_config: Optional config from saved checkpoint for architecture matching\n",
        "    '''\n",
        "    # Merge checkpoint config if provided (for exact architecture matching during sampling)\n",
        "    if checkpoint_config:\n",
        "        config = {**config, **checkpoint_config}\n",
        "\n",
        "    # Create encoder params if needed (for LatentMGN)\n",
        "    p_me = None\n",
        "    if config.get('requires_encoder'):\n",
        "        p_me = ModelParamsEncoder(\n",
        "            arch=\"skip\",\n",
        "            pretrained=False,\n",
        "            encWidth=config.get('enc_width', 32),\n",
        "            latentSize=config.get('latent_size', 32)\n",
        "        )\n",
        "\n",
        "    # Create latent params if needed (for LatentMGN)\n",
        "    p_ml = None\n",
        "    if config.get('requires_latent'):\n",
        "        p_ml = ModelParamsLatent(\n",
        "            arch=\"transformerMGN\",\n",
        "            pretrained=False,\n",
        "            width=1024,\n",
        "            layers=1,\n",
        "            dropout=0.0,\n",
        "            transTrainUnroll=True,\n",
        "            transTargetFull=False,\n",
        "            maxInputLen=30\n",
        "        )\n",
        "\n",
        "    # Create decoder params (required for all models)\n",
        "    p_md = ModelParamsDecoder(\n",
        "        arch=config['arch'],\n",
        "        pretrained=False,\n",
        "        decWidth=config.get('dec_width', 96),\n",
        "        fnoModes=config.get('fno_modes'),\n",
        "        diffSteps=config.get('diff_steps'),\n",
        "        diffSchedule=config.get('diff_schedule', 'linear'),\n",
        "        diffCondIntegration=config.get('diff_cond_integration', 'noisy'),\n",
        "        refinerStd=config.get('refiner_std'),\n",
        "        vae=config.get('vae', False),\n",
        "        n_sensors=config.get('n_sensors'),\n",
        "        training_stage=config.get('training_stage')\n",
        "    )\n",
        "\n",
        "    # Store DeepONet overrides in p_md for architecture matching\n",
        "    deeponet_overrides = {}\n",
        "    if 'deeponet' in config['arch'].lower():\n",
        "        deeponet_keys = ['branch_batch_norm', 'trunk_batch_norm', 'branch_layers', 'trunk_layers',\n",
        "                       'branch_activation', 'trunk_activation', 'branch_dropout', 'trunk_dropout']\n",
        "        for key in deeponet_keys:\n",
        "            if key in config:\n",
        "                setattr(p_md, key, config[key])\n",
        "                deeponet_overrides[key] = config[key]\n",
        "\n",
        "    return p_me, p_md, p_ml, deeponet_overrides\n",
        "\n",
        "def train_model(model_name, config):\n",
        "    '''Train a single model on TRA data'''\n",
        "    checkpoint_key = f\"{model_name}_tra\"\n",
        "    checkpoint_path = progress_dir / 'checkpoints' / f\"{checkpoint_key}.pt\"\n",
        "\n",
        "    # Check if done\n",
        "    if progress['training'].get(checkpoint_key) == 'complete' and checkpoint_path.exists():\n",
        "        print(f\"  \u2705 {model_name.upper()}: Already trained\")\n",
        "        return True\n",
        "\n",
        "    print(f\"  \ud83d\udd04 {model_name.upper()}: Training...\")\n",
        "\n",
        "    try:\n",
        "        # Get sequence length for this model\n",
        "        seq_len = config.get('sequence_length', [2, 2])\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = TurbulenceDataset(\n",
        "            name=f\"TRA_{model_name}\",\n",
        "            dataDirs=[\"data\"],\n",
        "            filterTop=TRA_CONFIG['filter_top'],\n",
        "            filterSim=TRA_CONFIG['filter_sim'],\n",
        "            filterFrame=TRA_CONFIG['filter_frame'],\n",
        "            sequenceLength=[seq_len],\n",
        "            randSeqOffset=True,\n",
        "            simFields=TRA_CONFIG['sim_fields'],\n",
        "            simParams=TRA_CONFIG['sim_params'],\n",
        "            printLevel=\"none\"\n",
        "        )\n",
        "\n",
        "        # DeepONet models with BatchNorm require batch_size >= 2\n",
        "        model_batch_size = BATCH_SIZE\n",
        "        if 'deeponet' in config['arch'].lower() and BATCH_SIZE < 2:\n",
        "            model_batch_size = 2\n",
        "            print(f\"     Note: Using batch_size={model_batch_size} for DeepONet (BatchNorm requirement)\")\n",
        "\n",
        "        # Create params\n",
        "        p_d = DataParams(\n",
        "            batch=model_batch_size,\n",
        "            augmentations=[\"normalize\"],\n",
        "            sequenceLength=seq_len,\n",
        "            randSeqOffset=True,\n",
        "            dataSize=[128, 64],  # Match autoreg reference\n",
        "            dimension=2,\n",
        "            simFields=TRA_CONFIG['sim_fields'],\n",
        "            simParams=TRA_CONFIG['sim_params'],\n",
        "            normalizeMode=TRA_CONFIG['normalize_mode']\n",
        "        )\n",
        "\n",
        "        # GPU-friendly training parameters: higher epochs, batch size, and learning rate decay\n",
        "        p_t = TrainingParams(epochs=EPOCHS, lr=0.0001, expLrGamma=0.995)\n",
        "\n",
        "        # Configure loss with LSIM for better perceptual quality on GPU\n",
        "        p_l = LossParams(recMSE=0.0, predMSE=1.0, predLSIM=1.0)\n",
        "        # Optional: Enable TNO relative L2 loss\n",
        "        # p_l = LossParams(recMSE=0.0, predMSE=1.0, predLSIM=1.0, tno_lp_loss=1.0)\n",
        "\n",
        "        # Create model params using helper function\n",
        "        p_me, p_md, p_ml, deeponet_overrides = create_model_params(config)\n",
        "\n",
        "        # Handle pretrained prior loading for NO+DM models\n",
        "        pretrain_path = \"\"\n",
        "        if config.get('load_pretrained_prior') and config.get('prior_checkpoint_key'):\n",
        "            prior_checkpoint = progress_dir / 'checkpoints' / f\"{config['prior_checkpoint_key']}.pt\"\n",
        "            if prior_checkpoint.exists():\n",
        "                pretrain_path = str(prior_checkpoint)\n",
        "                print(f\"     Loading pretrained prior from: {prior_checkpoint.name}\")\n",
        "                p_md.pretrained = True\n",
        "            else:\n",
        "                print(f\"     Warning: Pretrained prior not found: {prior_checkpoint.name}\")\n",
        "                print(f\"     Continuing with random initialization\")\n",
        "\n",
        "        # Create model\n",
        "        model = PredictionModel(p_d, p_t, p_l, p_me, p_md, p_ml, pretrain_path, useGPU=torch.cuda.is_available())\n",
        "\n",
        "        # Apply transforms\n",
        "        transforms = Transforms(p_d)\n",
        "        dataset.transform = transforms\n",
        "\n",
        "        # Create data loader\n",
        "        train_loader = DataLoader(dataset, batch_size=p_d.batch, shuffle=True,\n",
        "                                  drop_last=True, num_workers=0)\n",
        "\n",
        "        # Setup training using Trainer class from codebase\n",
        "        from src.core.training.trainer import Trainer\n",
        "        from src.core.training.loss_history import LossHistory\n",
        "\n",
        "        # Try to import SummaryWriter, fall back to dummy if tensorboard has issues\n",
        "        try:\n",
        "            from torch.utils.tensorboard import SummaryWriter\n",
        "            writer = SummaryWriter(log_dir=str(progress_dir / 'logs' / model_name))\n",
        "        except (ImportError, TypeError) as e:\n",
        "            class DummyWriter:\n",
        "                def add_scalar(self, *args, **kwargs): pass\n",
        "                def flush(self): pass\n",
        "                def close(self): pass\n",
        "            writer = DummyWriter()\n",
        "\n",
        "        # Setup optimizer and scheduler\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=p_t.lr)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            patience=5,\n",
        "            factor=p_t.expLrGamma\n",
        "        )\n",
        "        criterion = PredictionLoss(p_l, p_d.dimension, p_d.simFields, useGPU=torch.cuda.is_available())\n",
        "\n",
        "        # Create training history tracker with Rich progress bar\n",
        "        train_history = LossHistory(\n",
        "            \"_train\", \"Training\", writer, len(train_loader),\n",
        "            0, 1, printInterval=1, logInterval=1, simFields=p_d.simFields,\n",
        "            use_rich_progress=True, total_epochs=p_t.epochs,\n",
        "            model_name=model_name.upper(), loss_params=p_l\n",
        "        )\n",
        "\n",
        "        # Create Trainer with checkpoint support\n",
        "        trainer = Trainer(\n",
        "            model, train_loader, optimizer, lr_scheduler, criterion,\n",
        "            train_history, writer, p_d, p_t,\n",
        "            checkpoint_path=str(checkpoint_path),\n",
        "            checkpoint_frequency=max(1, p_t.epochs // 5),  # Save 5 checkpoints during training\n",
        "            min_epoch_for_scheduler=10,  # Start LR scheduling earlier on Colab\n",
        "            tno_teacher_forcing_ratio=config.get('tno_teacher_forcing_ratio', 0.0)  # TNO teacher forcing ratio\n",
        "        )\n",
        "\n",
        "        print(f\"     Training {p_t.epochs} epochs on {len(dataset)} samples using Trainer class...\")\n",
        "\n",
        "        # Training loop using Trainer.trainingStep()\n",
        "        for epoch in range(p_t.epochs):\n",
        "            trainer.trainingStep(epoch)\n",
        "\n",
        "        # Save checkpoint with enhanced config for architecture reproduction\n",
        "        enhanced_config = config.copy()\n",
        "\n",
        "        # For DeepONet models, save the actual architecture config used\n",
        "        if 'deeponet' in config['arch'].lower():\n",
        "            enhanced_config.update(deeponet_overrides)\n",
        "\n",
        "        # For NO+DM models, also capture prior architecture details if available\n",
        "        if config.get('load_pretrained_prior') and config.get('prior_checkpoint_key'):\n",
        "            prior_checkpoint_path = progress_dir / 'checkpoints' / f\"{config['prior_checkpoint_key']}.pt\"\n",
        "            if prior_checkpoint_path.exists():\n",
        "                try:\n",
        "                    prior_checkpoint = torch.load(prior_checkpoint_path, map_location='cpu', weights_only=False)\n",
        "                    prior_config = prior_checkpoint.get('config', {})\n",
        "                    # Merge prior architecture details (e.g., DeepONet BatchNorm flags)\n",
        "                    arch_keys = ['branch_batch_norm', 'trunk_batch_norm', 'branch_layers', 'trunk_layers',\n",
        "                                'branch_activation', 'trunk_activation', 'branch_dropout', 'trunk_dropout',\n",
        "                                'fno_modes', 'n_sensors']\n",
        "                    for key in arch_keys:\n",
        "                        if key in prior_config and key not in enhanced_config:\n",
        "                            enhanced_config[key] = prior_config[key]\n",
        "                except Exception as e:\n",
        "                    print(f\"     Warning: Could not load prior config: {e}\")\n",
        "\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': p_t.epochs,\n",
        "            'config': enhanced_config\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        progress['training'][checkpoint_key] = 'complete'\n",
        "        with open(progress_file, 'w') as f:\n",
        "            json.dump(progress, f, indent=2)\n",
        "\n",
        "        print(f\"     \u2705 Complete!\")\n",
        "\n",
        "        # Cleanup\n",
        "        train_history.cleanup()  # Stop Rich progress bar\n",
        "        del model, optimizer, dataset, train_loader\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"     \u274c Failed: {str(e)[:200]}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        progress['training'][checkpoint_key] = 'failed'\n",
        "        with open(progress_file, 'w') as f:\n",
        "            json.dump(progress, f, indent=2)\n",
        "        return False\n",
        "\n",
        "# Train all models\n",
        "success_count = 0\n",
        "for model_name, config in MODELS.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"\ud83d\udd2c Model: {model_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    if train_model(model_name, config):\n",
        "        success_count += 1\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"\u2705 Training Complete: {success_count}/{len(MODELS)} models trained\")\n",
        "print(f\"{'='*60}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Sampling Phase - Generate Predictions\n",
        "\n",
        "def sample_model(model_name: str, config: dict):\n",
        "    \"\"\"Generate predictions from a trained model\"\"\"\n",
        "    checkpoint_key = f\"{model_name}_tra\"\n",
        "    checkpoint_path = progress_dir / 'checkpoints' / f\"{checkpoint_key}.pt\"\n",
        "    sample_output_path = progress_dir / 'sampling' / f\"{checkpoint_key}.npz\"\n",
        "\n",
        "    # Check if model is trained\n",
        "    if not checkpoint_path.exists():\n",
        "        print(f\"  \u26a0\ufe0f  {model_name.upper()}: No checkpoint found, skipping sampling\")\n",
        "        return False\n",
        "\n",
        "    # Check if already sampled\n",
        "    if progress.get('sampling', {}).get(checkpoint_key) == 'complete' and sample_output_path.exists():\n",
        "        print(f\"  \u2705 {model_name.upper()}: Already sampled\")\n",
        "        return True\n",
        "\n",
        "    print(f\"  \ud83d\udd04 {model_name.upper()}: Generating predictions...\")\n",
        "\n",
        "    try:\n",
        "        # Create test dataset\n",
        "        test_dataset = TurbulenceDataset(\n",
        "            name=f\"TRA_test_{model_name}\",\n",
        "            dataDirs=[\"data\"],\n",
        "            filterTop=TRA_CONFIG['filter_top'],\n",
        "            filterSim=[(0, 3)],  # Different sims for testing\n",
        "            filterFrame=[(500, 750)],  # Different frames for testing\n",
        "            sequenceLength=[[60, 2]],\n",
        "            randSeqOffset=False,\n",
        "            simFields=TRA_CONFIG['sim_fields'],\n",
        "            simParams=TRA_CONFIG['sim_params'],\n",
        "            printLevel=\"none\"\n",
        "        )\n",
        "\n",
        "        # Create params\n",
        "        p_d = DataParams(\n",
        "            batch=1,\n",
        "            augmentations=[\"normalize\"],\n",
        "            sequenceLength=[60, 2],\n",
        "            randSeqOffset=False,\n",
        "            dataSize=[128, 64],\n",
        "            dimension=2,\n",
        "            simFields=TRA_CONFIG['sim_fields'],\n",
        "            simParams=TRA_CONFIG['sim_params'],\n",
        "            normalizeMode=TRA_CONFIG['normalize_mode']\n",
        "        )\n",
        "\n",
        "        p_t = TrainingParams(epochs=1, lr=0.0001)\n",
        "        p_l = LossParams(recMSE=0.0, predMSE=1.0)\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu', weights_only=False)\n",
        "        checkpoint_config = checkpoint.get('config', {})\n",
        "\n",
        "        # Create model\n",
        "        p_me, p_md, p_ml, deeponet_overrides = create_model_params(config, checkpoint_config)\n",
        "        model = PredictionModel(p_d, p_t, p_l, p_me, p_md, p_ml, \"\", useGPU=torch.cuda.is_available())\n",
        "\n",
        "        # Load weights\n",
        "        model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
        "        print(f\"     Loaded checkpoint from epoch {checkpoint.get('epoch', '?')}\")\n",
        "\n",
        "        # Create test loader\n",
        "        transforms = Transforms(p_d)\n",
        "        test_dataset.transform = transforms\n",
        "        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "        # Create Tester\n",
        "        from src.core.training.trainer import Tester\n",
        "\n",
        "        class DummyWriter:\n",
        "            def add_scalar(self, *args, **kwargs): pass\n",
        "            def add_image(self, *args, **kwargs): pass\n",
        "            def flush(self, *args, **kwargs): pass\n",
        "            def close(self): pass\n",
        "        writer = DummyWriter()\n",
        "\n",
        "        criterion = PredictionLoss(p_l, p_d.dimension, p_d.simFields, useGPU=torch.cuda.is_available())\n",
        "        test_history = LossHistory(\n",
        "            \"_test\", \"Testing\", writer, len(test_loader),\n",
        "            0, 1, printInterval=0, logInterval=0, simFields=p_d.simFields\n",
        "        )\n",
        "\n",
        "        tester = Tester(model, test_loader, criterion, test_history, p_t)\n",
        "\n",
        "        # Generate predictions\n",
        "        print(f\"     Generating predictions on {len(test_dataset)} test samples...\")\n",
        "        predictions = tester.generatePredictions(output_path=str(sample_output_path), model_name=model_name.upper(), show_progress=False)\n",
        "\n",
        "        # Save ground truth (only once)\n",
        "        ground_truth_path = progress_dir / 'sampling' / 'groundTruth.dict'\n",
        "        if not ground_truth_path.exists():\n",
        "            print(f\"     Saving ground truth data...\")\n",
        "            all_ground_truth = []\n",
        "            with torch.no_grad():\n",
        "                for sample in test_loader:\n",
        "                    all_ground_truth.append(sample[\"data\"])\n",
        "            ground_truth_tensor = torch.cat(all_ground_truth, dim=0)\n",
        "            torch.save({\"data\": ground_truth_tensor}, ground_truth_path)\n",
        "\n",
        "        # Update progress\n",
        "        if 'sampling' not in progress:\n",
        "            progress['sampling'] = {}\n",
        "        progress['sampling'][checkpoint_key] = 'complete'\n",
        "        with open(progress_file, 'w') as f:\n",
        "            json.dump(progress, f, indent=2)\n",
        "\n",
        "        print(f\"     \u2705 Complete! Shape: {predictions.shape}\")\n",
        "\n",
        "        # Cleanup\n",
        "        del model, test_dataset, test_loader\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"     \u274c Failed: {str(e)[:200]}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Sample all models\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83d\udd2e SAMPLING PHASE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sample_count = 0\n",
        "for model_name in MODELS.keys():\n",
        "    config = MODELS[model_name]\n",
        "    if sample_model(model_name, config):\n",
        "        sample_count += 1\n",
        "\n",
        "print(f\"\\n\u2705 Sampling complete: {sample_count}/{len(MODELS)} models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Plotting Phase - Generate Comparison Plots\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83d\udcca PLOTTING PHASE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Helper function to plot predictions vs ground truth\n",
        "def plot_comparison(model_name: str, time_step: int = 30):\n",
        "    \"\"\"Plot prediction vs ground truth for a specific model\"\"\"\n",
        "    checkpoint_key = f\"{model_name}_tra\"\n",
        "    sample_path = progress_dir / 'sampling' / f\"{checkpoint_key}.npz\"\n",
        "    gt_path = progress_dir / 'sampling' / 'groundTruth.dict'\n",
        "\n",
        "    if not sample_path.exists() or not gt_path.exists():\n",
        "        print(f\"  \u26a0\ufe0f  {model_name.upper()}: Missing data, skipping plot\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Load data\n",
        "        predictions = np.load(sample_path)['arr_0']  # [N, T, C, H, W]\n",
        "        ground_truth = torch.load(gt_path)['data'].numpy()  # [N, T, C, H, W]\n",
        "\n",
        "        # Select first sequence and specific time step\n",
        "        pred = predictions[0, time_step, :2]  # [C, H, W] - velocity fields only\n",
        "        gt = ground_truth[0, time_step, :2]\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
        "        fig.suptitle(f'{model_name.upper()} - Time Step {time_step}', fontsize=16)\n",
        "\n",
        "        # Velocity X\n",
        "        im0 = axes[0, 0].imshow(gt[0], cmap='RdBu_r', aspect='auto')\n",
        "        axes[0, 0].set_title('Ground Truth - Velocity X')\n",
        "        axes[0, 0].axis('off')\n",
        "        plt.colorbar(im0, ax=axes[0, 0])\n",
        "\n",
        "        im1 = axes[1, 0].imshow(pred[0], cmap='RdBu_r', aspect='auto')\n",
        "        axes[1, 0].set_title('Prediction - Velocity X')\n",
        "        axes[1, 0].axis('off')\n",
        "        plt.colorbar(im1, ax=axes[1, 0])\n",
        "\n",
        "        # Velocity Y\n",
        "        im2 = axes[0, 1].imshow(gt[1], cmap='RdBu_r', aspect='auto')\n",
        "        axes[0, 1].set_title('Ground Truth - Velocity Y')\n",
        "        axes[0, 1].axis('off')\n",
        "        plt.colorbar(im2, ax=axes[0, 1])\n",
        "\n",
        "        im3 = axes[1, 1].imshow(pred[1], cmap='RdBu_r', aspect='auto')\n",
        "        axes[1, 1].set_title('Prediction - Velocity Y')\n",
        "        axes[1, 1].axis('off')\n",
        "        plt.colorbar(im3, ax=axes[1, 1])\n",
        "\n",
        "        # Error maps\n",
        "        error_x = np.abs(gt[0] - pred[0])\n",
        "        error_y = np.abs(gt[1] - pred[1])\n",
        "\n",
        "        im4 = axes[0, 2].imshow(error_x, cmap='hot', aspect='auto')\n",
        "        axes[0, 2].set_title('Absolute Error - Velocity X')\n",
        "        axes[0, 2].axis('off')\n",
        "        plt.colorbar(im4, ax=axes[0, 2])\n",
        "\n",
        "        im5 = axes[1, 2].imshow(error_y, cmap='hot', aspect='auto')\n",
        "        axes[1, 2].set_title('Absolute Error - Velocity Y')\n",
        "        axes[1, 2].axis('off')\n",
        "        plt.colorbar(im5, ax=axes[1, 2])\n",
        "\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot\n",
        "        plot_dir = progress_dir / 'plots'\n",
        "        plot_dir.mkdir(exist_ok=True)\n",
        "        plot_path = plot_dir / f'{checkpoint_key}_comparison.png'\n",
        "        plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"  \u2705 {model_name.upper()}: Plot saved to {plot_path}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  \u274c {model_name.upper()}: Plotting failed - {str(e)[:100]}\")\n",
        "        return False\n",
        "\n",
        "# Plot all models with progress bar\n",
        "from rich.progress import Progress, SpinnerColumn, BarColumn, TextColumn, TimeElapsedColumn\n",
        "\n",
        "plot_count = 0\n",
        "with Progress(\n",
        "    SpinnerColumn(),\n",
        "    TextColumn(\"[bold blue]{task.description}\"),\n",
        "    BarColumn(),\n",
        "    TextColumn(\"[progress.percentage]{task.percentage:>3.0f}%\"),\n",
        "    TextColumn(\"\u2022\"),\n",
        "    TextColumn(\"{task.completed}/{task.total} models\"),\n",
        "    TimeElapsedColumn(),\n",
        ") as progress:\n",
        "    task = progress.add_task(\"Generating plots\", total=len(MODELS))\n",
        "\n",
        "    for model_name in MODELS.keys():\n",
        "        if plot_comparison(model_name):\n",
        "            plot_count += 1\n",
        "        progress.update(task, advance=1)\n",
        "\n",
        "print(f\"\\n\u2705 Plotting complete: {plot_count}/{len(MODELS)} models\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6b: Rollout Error Analysis - Stability over Autoregressive Rollout\n",
        "\n",
        "from src.analysis.plot_rollout_error import plot_rollout_error\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\ud83d\udcc8 ROLLOUT ERROR ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "rollout_plot_path = progress_dir / 'plots' / 'rollout_error.png'\n",
        "\n",
        "try:\n",
        "    plot_rollout_error(\n",
        "        prediction_folder=progress_dir / 'sampling',\n",
        "        model_names=list(MODELS.keys()),\n",
        "        output_path=rollout_plot_path,\n",
        "        metric='mse',\n",
        "        title='Autoregressive Rollout Stability (MSE vs Frame)'\n",
        "    )\n",
        "    print(f\"\\n\u2705 Rollout error plot saved: {rollout_plot_path}\")\n",
        "\n",
        "    # Display the plot\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(filename=str(rollout_plot_path)))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  \u26a0\ufe0f  Rollout error plot failed: {str(e)[:100]}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Summary\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca VERIFICATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with open(progress_file, 'r') as f:\n",
        "    progress = json.load(f)\n",
        "\n",
        "training_complete = sum(1 for v in progress.get('training', {}).values() if v == 'complete')\n",
        "training_failed = sum(1 for v in progress.get('training', {}).values() if v == 'failed')\n",
        "sampling_complete = sum(1 for v in progress.get('sampling', {}).values() if v == 'complete')\n",
        "\n",
        "print(f\"\\nTraining: {training_complete} complete, {training_failed} failed\")\n",
        "print(f\"Sampling: {sampling_complete} complete\")\n",
        "\n",
        "print(f\"\\nTraining Results:\")\n",
        "for key, status in progress.get('training', {}).items():\n",
        "    symbol = '\u2705' if status == 'complete' else '\u274c'\n",
        "    print(f\"  {symbol} {key}\")\n",
        "\n",
        "print(f\"\\nSampling Results:\")\n",
        "for key, status in progress.get('sampling', {}).items():\n",
        "    symbol = '\u2705' if status == 'complete' else '\u274c'\n",
        "    print(f\"  {symbol} {key}\")\n",
        "\n",
        "if training_complete > 0 and sampling_complete > 0:\n",
        "    print(f\"\\n\ud83c\udf89 SUCCESS! Full pipeline verified: Training \u2192 Sampling \u2192 Plotting\")\n",
        "elif training_complete > 0:\n",
        "    print(f\"\\n\u2705 Training complete. Run sampling and plotting cells to generate visualizations.\")\n",
        "else:\n",
        "    print(f\"\\n\u26a0\ufe0f  No models completed successfully.\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}