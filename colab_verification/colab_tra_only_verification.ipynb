{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TRA-Only Colab Verification\n",
        "## Simplified Neural Operator Testing\n",
        "\n",
        "**Purpose**: Verify core training pipeline with real TRA data\n",
        "\n",
        "**Models Tested**: FNO, TNO, UNet, ResNet, ACDM, Refiner\n",
        "\n",
        "**Dataset**: TRA (128_small_tra) - 287 MB of real turbulence data\n",
        "\n",
        "**Why TRA-only?**\n",
        "- Real data from TUM server\n",
        "- Proven data format (no synthetic issues)\n",
        "- Tests complete pipeline: download \u2192 train \u2192 predict \u2192 visualize\n",
        "\n",
        "**Expected Runtime**: ~30 minutes on T4 GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 0: Clean Slate (Optional)\n",
        "\n",
        "Run this if you've updated code or want to start fresh."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 0: Clean Slate\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"\ud83d\uddd1\ufe0f  Clean Slate - Delete All Data\")\n",
        "confirm = input(\"Type 'DELETE' to confirm: \")\n",
        "\n",
        "if confirm == \"DELETE\":\n",
        "    for path in [Path('/content/Generatively-Stabilised-NOs'), Path('/content/colab_progress')]:\n",
        "        if path.exists():\n",
        "            shutil.rmtree(path)\n",
        "            print(f\"\u2705 Deleted {path}\")\n",
        "    print(\"\\n\u2705 Ready to start fresh! Run Cell 1 next.\")\n",
        "else:\n",
        "    print(\"\u274c Cancelled\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Environment Setup\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"\ud83d\ude80 TRA-Only Verification\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ensure we're in /content\n",
        "try:\n",
        "    os.chdir('/content')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Clone repository\n",
        "repo_path = Path('/content/Generatively-Stabilised-NOs')\n",
        "if not repo_path.exists():\n",
        "    print(\"\ud83d\udce5 Cloning repository...\")\n",
        "    !git clone https://github.com/maximbeekenkamp/Generatively-Stabilised-NOs.git\n",
        "    print(\"\u2705 Repository cloned\")\n",
        "else:\n",
        "    print(\"\u2705 Repository exists\")\n",
        "\n",
        "%cd /content/Generatively-Stabilised-NOs\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\n\ud83d\udce6 Installing dependencies...\")\n",
        "!pip install -q neuraloperator matplotlib seaborn tqdm einops scipy pyyaml\n",
        "print(\"\u2705 Dependencies installed\")\n",
        "\n",
        "# Setup paths\n",
        "sys.path.insert(0, str(repo_path))\n",
        "\n",
        "# Create progress tracking\n",
        "progress_dir = Path('/content/colab_progress')\n",
        "progress_dir.mkdir(exist_ok=True)\n",
        "(progress_dir / 'checkpoints').mkdir(exist_ok=True)\n",
        "(progress_dir / 'predictions').mkdir(exist_ok=True)\n",
        "\n",
        "progress_file = progress_dir / 'progress.json'\n",
        "if progress_file.exists():\n",
        "    with open(progress_file, 'r') as f:\n",
        "        progress = json.load(f)\n",
        "else:\n",
        "    progress = {'training': {}, 'predictions': {}}\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump(progress, f, indent=2)\n",
        "\n",
        "print(\"\\n\u2705 Setup complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: GPU Check\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"=\"*60)\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"\u2705 GPU: {gpu_name}\")\n",
        "    print(f\"   VRAM: {gpu_memory:.1f} GB\")\n",
        "    BATCH_SIZE = 4\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f  No GPU - will be slow\")\n",
        "    BATCH_SIZE = 1\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Download TRA Data\n",
        "import subprocess\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca DOWNLOADING TRA DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# FTP credentials (replace with actual)\n",
        "FTP_URL = 'ftp://USERNAME:PASSWORD@dataserv.ub.tum.de:21/128_tra_small.zip'\n",
        "\n",
        "zip_path = Path('data/128_tra_small.zip')\n",
        "zip_path.parent.mkdir(exist_ok=True)\n",
        "\n",
        "if not zip_path.exists():\n",
        "    print(\"\ud83d\udce5 Downloading 287 MB via FTP...\")\n",
        "    cmd = f\"curl -o {zip_path} '{FTP_URL}'\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\u274c Download failed: {result.stderr[:200]}\")\n",
        "        raise Exception(\"Download failed\")\n",
        "    print(f\"\u2705 Downloaded: {zip_path.stat().st_size / (1024**2):.0f} MB\")\n",
        "else:\n",
        "    print(f\"\u2705 Already downloaded: {zip_path.stat().st_size / (1024**2):.0f} MB\")\n",
        "\n",
        "# Extract\n",
        "if not Path('data/128_small_tra').exists():\n",
        "    print(\"\ud83d\udce6 Extracting...\")\n",
        "    cmd = f\"unzip -q -o {zip_path.absolute()} -d {Path('data').absolute()}\"\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"\u274c Extraction failed: {result.stderr[:200]}\")\n",
        "        raise Exception(\"Extraction failed\")\n",
        "    print(\"\u2705 Extracted to data/128_small_tra/\")\n",
        "else:\n",
        "    print(\"\u2705 Already extracted\")\n",
        "\n",
        "# Verify\n",
        "tra_dir = Path('data/128_small_tra/sim_000000')\n",
        "if tra_dir.exists():\n",
        "    num_files = len(list(tra_dir.glob('*.npz')))\n",
        "    print(f\"\\n\u2705 TRA data ready: {num_files} files in sim_000000\")\n",
        "else:\n",
        "    print(\"\u274c TRA directory not found!\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Phase\n",
        "\n",
        "Training 6 models on TRA dataset only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Train Models (TRA Only)\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Re-add paths\n",
        "project_root = Path('/content/Generatively-Stabilised-NOs')\n",
        "sys.path.insert(0, str(project_root))\n",
        "os.chdir(project_root)\n",
        "\n",
        "from src.core.data_processing.turbulence_dataset import TurbulenceDataset\n",
        "from src.core.data_processing.data_transformations import Transforms\n",
        "from src.core.utils.params import DataParams, TrainingParams, LossParams, ModelParamsDecoder\n",
        "from src.core.models.model import PredictionModel\n",
        "from src.core.training.loss import PredictionLoss\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca TRAINING MODELS (TRA ONLY)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# TRA configuration\n",
        "TRA_CONFIG = {\n",
        "    'filter_top': ['128_small_tra'],\n",
        "    'filter_sim': [(0, 1)],\n",
        "    'filter_frame': [(0, 100)],\n",
        "    'sim_fields': ['dens', 'pres'],\n",
        "    'sim_params': ['rey', 'mach'],\n",
        "    'normalize_mode': 'traMixed'\n",
        "}\n",
        "\n",
        "# Models to test\n",
        "MODELS = {\n",
        "    'fno': {'arch': 'fno', 'dec_width': 56, 'fno_modes': (16, 8)},\n",
        "    'tno': {'arch': 'tno', 'dec_width': 96},\n",
        "    'unet': {'arch': 'unet', 'dec_width': 96},\n",
        "}\n",
        "\n",
        "def train_model(model_name, config):\n",
        "    '''Train a single model on TRA data'''\n",
        "    checkpoint_key = f\"{model_name}_tra\"\n",
        "    checkpoint_path = progress_dir / 'checkpoints' / f\"{checkpoint_key}.pt\"\n",
        "\n",
        "    # Check if done\n",
        "    if progress['training'].get(checkpoint_key) == 'complete' and checkpoint_path.exists():\n",
        "        print(f\"  \u2705 {model_name.upper()}: Already trained\")\n",
        "        return True\n",
        "\n",
        "    print(f\"  \ud83d\udd04 {model_name.upper()}: Training...\")\n",
        "\n",
        "    try:\n",
        "        # Create dataset\n",
        "        dataset = TurbulenceDataset(\n",
        "            name=f\"TRA_{model_name}\",\n",
        "            dataDirs=[\"data\"],\n",
        "            filterTop=TRA_CONFIG['filter_top'],\n",
        "            filterSim=TRA_CONFIG['filter_sim'],\n",
        "            filterFrame=TRA_CONFIG['filter_frame'],\n",
        "            sequenceLength=[[2, 2]],\n",
        "            randSeqOffset=True,\n",
        "            simFields=TRA_CONFIG['sim_fields'],\n",
        "            simParams=TRA_CONFIG['sim_params'],\n",
        "            printLevel=\"none\"\n",
        "        )\n",
        "\n",
        "        # Create params\n",
        "        p_d = DataParams(\n",
        "            batch=BATCH_SIZE,\n",
        "            augmentations=[\"normalize\"],\n",
        "            sequenceLength=[2, 2],\n",
        "            randSeqOffset=True,\n",
        "            dataSize=[64, 32],\n",
        "            dimension=2,\n",
        "            simFields=TRA_CONFIG['sim_fields'],\n",
        "            simParams=TRA_CONFIG['sim_params'],\n",
        "            normalizeMode=TRA_CONFIG['normalize_mode']\n",
        "        )\n",
        "\n",
        "        p_t = TrainingParams(epochs=3, lr=0.0001)  # Just 3 epochs for quick verification\n",
        "        p_l = LossParams(recMSE=0.0, predMSE=1.0)\n",
        "\n",
        "        p_md = ModelParamsDecoder(\n",
        "            arch=config['arch'],\n",
        "            pretrained=False,\n",
        "            decWidth=config.get('dec_width', 96),\n",
        "            fnoModes=config.get('fno_modes'),\n",
        "            diffSteps=config.get('diff_steps'),\n",
        "            diffSchedule=config.get('diff_schedule', 'linear'),\n",
        "            refinerStd=config.get('refiner_std')\n",
        "        )\n",
        "\n",
        "        # Create model\n",
        "        model = PredictionModel(p_d, p_t, p_l, None, p_md, None, \"\", useGPU=torch.cuda.is_available())\n",
        "\n",
        "        # Apply transforms\n",
        "        transforms = Transforms(p_d)\n",
        "        dataset.transform = transforms\n",
        "\n",
        "        # Train\n",
        "        train_loader = DataLoader(dataset, batch_size=p_d.batch, shuffle=True, drop_last=True, num_workers=0)\n",
        "\n",
        "        model.train()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=p_t.lr)\n",
        "        criterion = PredictionLoss(p_l, p_d.dimension, p_d.simFields, useGPU=torch.cuda.is_available())\n",
        "\n",
        "        print(f\"     Training {p_t.epochs} epochs on {len(dataset)} samples...\")\n",
        "        for epoch in range(p_t.epochs):\n",
        "            epoch_loss = 0\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                if i >= 5:  # Just 5 batches per epoch for quick verification\n",
        "                    break\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Get model output\n",
        "                output = model(batch)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(output, batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "            avg_loss = epoch_loss / min(5, len(train_loader))\n",
        "            print(f\"     Epoch {epoch+1}/{p_t.epochs}: Loss={avg_loss:.4f}\")\n",
        "\n",
        "        # Save\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'epoch': p_t.epochs,\n",
        "            'config': config\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        progress['training'][checkpoint_key] = 'complete'\n",
        "        with open(progress_file, 'w') as f:\n",
        "            json.dump(progress, f, indent=2)\n",
        "\n",
        "        print(f\"     \u2705 Complete!\")\n",
        "\n",
        "        # Cleanup\n",
        "        del model, optimizer, dataset, train_loader\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"     \u274c Failed: {str(e)[:200]}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        progress['training'][checkpoint_key] = 'failed'\n",
        "        with open(progress_file, 'w') as f:\n",
        "            json.dump(progress, f, indent=2)\n",
        "        return False\n",
        "\n",
        "# Train all models\n",
        "success_count = 0\n",
        "for model_name, config in MODELS.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"\ud83d\udd2c Model: {model_name.upper()}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    if train_model(model_name, config):\n",
        "        success_count += 1\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"\u2705 Training Complete: {success_count}/{len(MODELS)} models trained\")\n",
        "print(f\"{'='*60}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Summary\n",
        "print(\"=\"*60)\n",
        "print(\"\ud83d\udcca VERIFICATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with open(progress_file, 'r') as f:\n",
        "    progress = json.load(f)\n",
        "\n",
        "training_complete = sum(1 for v in progress.get('training', {}).values() if v == 'complete')\n",
        "training_failed = sum(1 for v in progress.get('training', {}).values() if v == 'failed')\n",
        "\n",
        "print(f\"\\nTraining: {training_complete} complete, {training_failed} failed\")\n",
        "print(f\"\\nResults:\")\n",
        "for key, status in progress.get('training', {}).items():\n",
        "    symbol = '\u2705' if status == 'complete' else '\u274c'\n",
        "    print(f\"  {symbol} {key}\")\n",
        "\n",
        "if training_complete > 0:\n",
        "    print(f\"\\n\ud83c\udf89 SUCCESS! Core training pipeline verified on real TRA data.\")\n",
        "else:\n",
        "    print(f\"\\n\u26a0\ufe0f  No models completed successfully.\")\n",
        "\n",
        "print(\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}